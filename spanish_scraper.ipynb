{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce0c97",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "selenium_spain_adalimumab_scraper_v3.py\n",
    "\n",
    "- Fills the exact search box input (id contains 'boxSearch') with 'adalimumab'\n",
    "- Clicks the exact submit button (id contains 'botonSubmitSearch')\n",
    "- Collects detail-page URLs matching 'adalimumab' (prefers 'detalle_licitacion')\n",
    "- Pages through results using the input[type=image] next control (id contains 'tableResultSearch' / 'j_id_1g')\n",
    "\n",
    "Additional behavior:\n",
    "- For every collected detail_url the script will open the detail page in a new tab,\n",
    "  try to extract the \"file name\" displayed on that page (span with id containing 'text_Expediente'\n",
    "  or adjacent label/value pairs), and include that file name in the output CSV.\n",
    "  This is useful because the tender id is not always visible in the results table.\n",
    "\n",
    "Output:\n",
    "  - collected_adalimumab_urls.csv with columns: file_name, tender_id, detail_url\n",
    "\n",
    "CONFIG:\n",
    "  - CHROME_DRIVER_PATH if chromedriver is not on PATH\n",
    "  - HEADLESS=False to visually debug\n",
    "\"\"\"\n",
    "import time, re, logging, csv\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "SEARCH_URL = \"https://contrataciondelestado.es/wps/portal/plataforma/buscador/!ut/p/z1/jZBNb4MwDIZ_yw69xk4oFI7hoyxTp5WihJLLlKG0ZYKCWrTt549x2WkM32w9zyvboOEI-mo-6rMZ6u5qmrEvtffKVZJxETiIQRQiO_D1IZWMYbCBYgLC3Pd5SDmiI33kXpZFQYwj4IBe4rtOtFZPau_lIkUUj9t4J6mLKfOW-fhHcVzmzwB6Pr4APSFzF_yXUY47bH5_-CKfOYodFVmyZxQjCvlPRtW1pH5ryclU9k767jY0diBKJIWIoVzh12VomxXeq663uTW36qJq-0mmMfStlPKI725_fvgGrHjPHg!!/dz/d5/L2dBISEvZ0FBIS9nQSEh/p0/IZ7_BS88AB1A0OUMA0IL1IQEP210C1=CZ6_AVEQAI93009CB02RA4RGU22097=LA0=Ecom.ibm.faces.portlet.VIEWID!QCPxhtmlQCPscopeSearchView.xhtml==/#Z7_BS88AB1A0OUMA0IL1IQEP210C1\"\n",
    "SEARCH_QUERY = \"adalimumab\"\n",
    "HEADLESS = True\n",
    "CHROME_DRIVER_PATH = None  # set to path if needed\n",
    "MAX_PAGES = 200\n",
    "WAIT = 1.0\n",
    "ADALIMUMAB_PATTERN = re.compile(r\"adalimumab\", re.I)\n",
    "OUTPUT_CSV = \"collected_adalimumab_urls.csv\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------- utility to extract tender id ----------------\n",
    "def extract_tender_id_from_text(text):\n",
    "    \"\"\"\n",
    "    Heuristic extraction of tender id / file number from visible text:\n",
    "    - looks for patterns like 2025/006468\n",
    "    - looks for \"Expediente ...\" patterns\n",
    "    - fallback: any token starting with 20xx\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r\"\\b(20\\d{2}\\/[0-9A-Za-z\\-\\_]{2,40})\\b\", text)\n",
    "    if m:\n",
    "        return re.sub(r\"[^\\w\\-_\\.]\", \"_\", m.group(1))\n",
    "    m2 = re.search(r\"(?:Expediente|Expdte|Expediente nº|Expediente n[oº]\\.?)\\W*[:\\-]?\\s*([A-Z0-9\\/\\-\\._]{4,60})\", text, flags=re.I)\n",
    "    if m2:\n",
    "        return re.sub(r\"[^\\w\\-_\\.]\", \"_\", m2.group(1))\n",
    "    m3 = re.search(r\"\\b(20\\d{2}[^\\s,;]{2,40})\\b\", text)\n",
    "    if m3:\n",
    "        return re.sub(r\"[^\\w\\-_\\.]\", \"_\", m3.group(1))\n",
    "    return None\n",
    "\n",
    "# ------------- helpers -------------\n",
    "def make_driver(headless=HEADLESS):\n",
    "    \"\"\"\n",
    "    Create Chrome webdriver with minimal options. Caller can set CHROME_DRIVER_PATH if necessary.\n",
    "    \"\"\"\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--window-size=1400,1000\")\n",
    "    if CHROME_DRIVER_PATH:\n",
    "        driver = webdriver.Chrome(executable_path=CHROME_DRIVER_PATH, options=opts)\n",
    "    else:\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "    driver.implicitly_wait(5)\n",
    "    return driver\n",
    "\n",
    "def ensure_search_box_and_submit(driver, query, wait_seconds=10):\n",
    "    \"\"\"\n",
    "    Find the search input by id containing 'boxSearch', fill it with query,\n",
    "    and click the submit button id containing 'botonSubmitSearch'.\n",
    "    Returns True if submission attempted (even if results take time).\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, wait_seconds)\n",
    "    try:\n",
    "        try:\n",
    "            input_el = driver.find_element(By.XPATH, \"//*[contains(@id, 'boxSearch') and (@type='text' or @type='search')]\")\n",
    "        except Exception:\n",
    "            input_el = driver.find_element(By.CSS_SELECTOR, \"input[type='text'], input[type='search']\")\n",
    "        try:\n",
    "            input_el.clear()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            input_el.send_keys(query)\n",
    "        except Exception:\n",
    "            driver.execute_script(\"arguments[0].value = arguments[1]; arguments[0].dispatchEvent(new Event('input'));\", input_el, query)\n",
    "        time.sleep(0.4)\n",
    "        try:\n",
    "            submit_el = driver.find_element(By.XPATH, \"//*[contains(@id, 'botonSubmitSearch') and (@type='submit' or @type='button')]\")\n",
    "        except Exception:\n",
    "            try:\n",
    "                submit_el = driver.find_element(By.XPATH, \"//button[contains(normalize-space(.),'Buscar') or contains(., 'Buscar')]\")\n",
    "            except Exception:\n",
    "                submit_el = None\n",
    "        if submit_el:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", submit_el)\n",
    "            time.sleep(0.2)\n",
    "            try:\n",
    "                submit_el.click()\n",
    "            except Exception:\n",
    "                try:\n",
    "                    input_el.send_keys(Keys.ENTER)\n",
    "                except Exception:\n",
    "                    driver.execute_script(\"arguments[0].click();\", submit_el)\n",
    "            time.sleep(1.0)\n",
    "            try:\n",
    "                wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, \"table tbody tr\")) > 0)\n",
    "            except Exception:\n",
    "                pass\n",
    "            return True\n",
    "        else:\n",
    "            logger.warning(\"Search submit element not found.\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.exception(\"ensure_search_box_and_submit failed: %s\", e)\n",
    "        return False\n",
    "\n",
    "def rows_selector_try(driver):\n",
    "    \"\"\"\n",
    "    Try several selectors to find meaningful result rows. Return list of WebElements.\n",
    "    \"\"\"\n",
    "    selectors = [\n",
    "        \"table tbody tr\",\n",
    "        \".ui-datatable-data tr\",\n",
    "        \"#tableResultSearch tbody tr\",\n",
    "        \".searchResults table tbody tr\",\n",
    "    ]\n",
    "    for sel in selectors:\n",
    "        try:\n",
    "            elems = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "            meaningful = [e for e in elems if e.text.strip()]\n",
    "            if meaningful:\n",
    "                return meaningful\n",
    "        except Exception:\n",
    "            continue\n",
    "    try:\n",
    "        elems = driver.find_elements(By.TAG_NAME, \"tr\")\n",
    "        return [e for e in elems if e.text.strip()]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def prefer_detail_link_from_row(row_el, current_page_url):\n",
    "    \"\"\"\n",
    "    From a results row, prefer a link that points to detalle_licitacion (detail page).\n",
    "    If not present, pick a reasonable fallback (pdf/document).\n",
    "    \"\"\"\n",
    "    anchors = row_el.find_elements(By.TAG_NAME, \"a\")\n",
    "    fallback_href = None\n",
    "    pdf_href = None\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            href = a.get_attribute(\"href\")\n",
    "            text = (a.text or \"\").strip()\n",
    "            if href:\n",
    "                href = urljoin(current_page_url, href)\n",
    "                href_l = href.lower()\n",
    "                if \"detalle_licitacion\" in href_l or \"deeplink:detalle_licitacion\" in href_l:\n",
    "                    return href\n",
    "                if \"detalle de la licitación\" in text.lower() or \"detalle de la licitaci\" in text.lower():\n",
    "                    return href\n",
    "                if \"getdocumentbyidservlet\" in href_l or href_l.endswith(\".pdf\"):\n",
    "                    pdf_href = pdf_href or href\n",
    "                fallback_href = fallback_href or href\n",
    "        except Exception:\n",
    "            continue\n",
    "    if fallback_href:\n",
    "        return fallback_href\n",
    "    if pdf_href:\n",
    "        return pdf_href\n",
    "    return None\n",
    "\n",
    "RANGE_RE = re.compile(r\"(\\d{1,3})\\s*-\\s*(\\d{1,3})\\s*(?:de|of)\\s*(\\d{1,6})\", flags=re.I)\n",
    "\n",
    "def _get_results_range_from_page(driver):\n",
    "    \"\"\"\n",
    "    Parse a visible \"start - end of total\" text on the page and return tuple (start, end, total) when possible.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        body_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "    except Exception:\n",
    "        body_text = \"\"\n",
    "    m = RANGE_RE.search(body_text)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def click_next_exact(driver, timeout=12, retry_if_no_change=True):\n",
    "    \"\"\"\n",
    "    Click the 'Next' control (targets input[type=image] with id suffix ':j_id_1g') and wait until page moves forward.\n",
    "    Returns True when the page advanced; False otherwise.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, timeout)\n",
    "    before_range = _get_results_range_from_page(driver)\n",
    "    before_start = before_range[0] if before_range else None\n",
    "    xpath_next = \"//input[@type='image' and contains(@id, 'tableResultSearch') and contains(@id, ':j_id_1g')]\"\n",
    "    try:\n",
    "        next_btn = driver.find_element(By.XPATH, xpath_next)\n",
    "    except Exception:\n",
    "        try:\n",
    "            cand = driver.find_elements(By.XPATH, \"//input[@type='image' and contains(@src,'NextButton.gif')]\")\n",
    "            next_btn = None\n",
    "            for c in cand:\n",
    "                try:\n",
    "                    iid = c.get_attribute(\"id\") or \"\"\n",
    "                    if \":j_id_1g\" in iid or iid.endswith(\"j_id_1g\"):\n",
    "                        next_btn = c\n",
    "                        break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if next_btn is None and cand:\n",
    "                for c in cand:\n",
    "                    try:\n",
    "                        iid = c.get_attribute(\"id\") or \"\"\n",
    "                        if \":j_id_1e\" not in iid:\n",
    "                            next_btn = c\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception:\n",
    "            next_btn = None\n",
    "    if not next_btn:\n",
    "        logger.info(\"Next button (exact) not found by id/xpath.\")\n",
    "        return False\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_btn)\n",
    "    except Exception:\n",
    "        pass\n",
    "    def do_click(el):\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].click();\", el)\n",
    "        except Exception:\n",
    "            try:\n",
    "                el.click()\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Both JS click and .click() failed: %s\", e)\n",
    "                raise\n",
    "    for attempt in (1, 2) if retry_if_no_change else (1,):\n",
    "        try:\n",
    "            do_click(next_btn)\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Clicking Next failed on attempt %d: %s\", attempt, e)\n",
    "            time.sleep(0.5)\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.XPATH, xpath_next)\n",
    "                do_click(next_btn)\n",
    "            except Exception as e2:\n",
    "                logger.debug(\"Re-finding and clicking next failed: %s\", e2)\n",
    "                return False\n",
    "        try:\n",
    "            def range_moved(drv):\n",
    "                new = _get_results_range_from_page(drv)\n",
    "                if not new:\n",
    "                    try:\n",
    "                        rows = drv.find_elements(By.CSS_SELECTOR, \"table tbody tr\")\n",
    "                        if rows and len(rows) > 0:\n",
    "                            first = next((r.text.strip() for r in rows if r.text.strip()), \"\")\n",
    "                            return first != \"\"\n",
    "                    except Exception:\n",
    "                        return False\n",
    "                else:\n",
    "                    new_start = new[0]\n",
    "                    if before_start is None:\n",
    "                        return new_start != (before_start or 1)\n",
    "                    return new_start > (before_start or 0)\n",
    "            WebDriverWait(driver, timeout).until(range_moved)\n",
    "            time.sleep(0.4)\n",
    "            after_range = _get_results_range_from_page(driver)\n",
    "            if before_start is not None and after_range is not None:\n",
    "                if after_range[0] > before_start:\n",
    "                    logger.info(\"Successfully advanced to next page: %s -> %s\", before_start, after_range[0])\n",
    "                    return True\n",
    "                else:\n",
    "                    logger.warning(\"After click, start did not increase: before=%s after=%s\", before_start, after_range[0])\n",
    "            else:\n",
    "                logger.info(\"Advanced page (couldn't parse numeric ranges, but content updated).\")\n",
    "                return True\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Timed out waiting for page change after clicking Next (attempt %d).\", attempt)\n",
    "            time.sleep(0.5)\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.XPATH, xpath_next)\n",
    "            except Exception:\n",
    "                break\n",
    "    logger.info(\"Failed to advance to next page after clicking Next.\")\n",
    "    return False\n",
    "\n",
    "# ------------- new small helper to probe detail page for \"file name\" -------------\n",
    "def extract_file_name_from_detail(driver, url, wait_seconds=1.5):\n",
    "    \"\"\"\n",
    "    Open detail URL in a new tab, try to extract the 'file name' shown on the detail page.\n",
    "    Looks for:\n",
    "      - span with id containing 'text_Expediente'\n",
    "      - sibling span following a label span with id containing 'label_Expediente'\n",
    "      - span whose title/text contains 'Expediente' label nearby\n",
    "    Returns the found string or None.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    original_handle = None\n",
    "    try:\n",
    "        original_handle = driver.current_window_handle\n",
    "    except Exception:\n",
    "        original_handle = None\n",
    "\n",
    "    try:\n",
    "        # open new tab\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        handles = driver.window_handles\n",
    "        new_handle = handles[-1]\n",
    "        driver.switch_to.window(new_handle)\n",
    "        driver.get(url)\n",
    "        time.sleep(wait_seconds)\n",
    "\n",
    "        # 1) direct id-based candidate(s)\n",
    "        try:\n",
    "            elems = driver.find_elements(By.XPATH, \"//*[contains(@id,'text_Expediente')]\")\n",
    "            for e in elems:\n",
    "                val = (e.get_attribute(\"title\") or e.text or \"\").strip()\n",
    "                if val:\n",
    "                    result = val\n",
    "                    break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not result:\n",
    "            # 2) look for 'label_Expediente' then following sibling span (label/value pattern)\n",
    "            try:\n",
    "                labs = driver.find_elements(By.XPATH, \"//*[contains(@id,'label_Expediente')]\")\n",
    "                for lab in labs:\n",
    "                    try:\n",
    "                        sib = lab.find_element(By.XPATH, \"following-sibling::*[1]\")\n",
    "                        val = (sib.get_attribute(\"title\") or sib.text or \"\").strip()\n",
    "                        if val:\n",
    "                            result = val\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not result:\n",
    "            # 3) broader fallback: any span whose title/text looks like an expediente token (slash-separated or contains '/' sequences)\n",
    "            try:\n",
    "                spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "                for s in spans:\n",
    "                    txt = (s.get_attribute(\"title\") or s.text or \"\").strip()\n",
    "                    if txt and re.search(r\"\\d{2,4}\\/.+\\/.+\", txt):\n",
    "                        result = txt\n",
    "                        break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not result:\n",
    "            # 4) last resort: look for any element containing the word 'Expediente' nearby and take adjacent token\n",
    "            try:\n",
    "                labels = driver.find_elements(By.XPATH, \"//*[contains(translate(text(),'ABCDEFGHIJKLMNOPQRSTUVWXYZ','abcdefghijklmnopqrstuvwxyz'),'expediente')]\")\n",
    "                for lab in labels:\n",
    "                    txt = (lab.get_attribute(\"title\") or lab.text or \"\").strip()\n",
    "                    if txt and not txt.lower().startswith(\"expediente\"):\n",
    "                        result = txt\n",
    "                        break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.debug(\"extract_file_name_from_detail error for %s : %s\", url, e)\n",
    "    finally:\n",
    "        # close the tab and switch back (if possible)\n",
    "        try:\n",
    "            driver.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            if original_handle and original_handle in driver.window_handles:\n",
    "                driver.switch_to.window(original_handle)\n",
    "            elif driver.window_handles:\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return result\n",
    "\n",
    "# ------------- main -------------\n",
    "def collect_detail_urls(search_url, query, max_pages=MAX_PAGES, headless=HEADLESS):\n",
    "    \"\"\"\n",
    "    Main routine:\n",
    "      - open search page, submit query\n",
    "      - iterate results pages, collect detail links\n",
    "      - for each detail link, probe the detail page briefly to extract file_name if present\n",
    "      - return list of tuples (file_name, tender_id, detail_url)\n",
    "    \"\"\"\n",
    "    driver = make_driver(headless=headless)\n",
    "    wait = WebDriverWait(driver, 12)\n",
    "    driver.get(search_url)\n",
    "    time.sleep(1.5)\n",
    "    # try to dismiss cookie prompts\n",
    "    try:\n",
    "        for txt in [\"Aceptar\", \"Aceptar cookies\", \"ACEPTAR\", \"Aceptar las cookies\"]:\n",
    "            try:\n",
    "                btn = driver.find_element(By.XPATH, f\"//button[contains(., '{txt}') or contains(., '{txt.lower()}')]\")\n",
    "                if btn.is_displayed():\n",
    "                    btn.click()\n",
    "                    time.sleep(0.5)\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    ok = ensure_search_box_and_submit(driver, query)\n",
    "    if not ok:\n",
    "        logger.warning(\"Search submit did not appear to work (but continuing).\")\n",
    "\n",
    "    collected = []  # list of tuples (file_name, tender_id, detail_url)\n",
    "    page = 0\n",
    "    last_first_row_text = None\n",
    "\n",
    "    while True:\n",
    "        page += 1\n",
    "        logger.info(\"Processing results page %d\", page)\n",
    "        try:\n",
    "            wait.until(lambda d: len(rows_selector_try(d)) > 0)\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"No rows appeared on page %d (timeout).\", page)\n",
    "            break\n",
    "\n",
    "        rows = rows_selector_try(driver)\n",
    "        logger.info(\"Found %d rows on page %d\", len(rows), page)\n",
    "\n",
    "        current_url = driver.current_url\n",
    "\n",
    "        for r in rows:\n",
    "            try:\n",
    "                row_text = r.text or \"\"\n",
    "                if not ADALIMUMAB_PATTERN.search(row_text):\n",
    "                    continue\n",
    "                href = prefer_detail_link_from_row(r, current_url)\n",
    "                if not href:\n",
    "                    logger.info(\"Matched row text but no link found; skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # try to extract tender id from row text or anchor title\n",
    "                tid = extract_tender_id_from_text(row_text)\n",
    "                if not tid:\n",
    "                    try:\n",
    "                        a_with_title = r.find_element(By.XPATH, \".//a[@title and string-length(normalize-space(@title))>0]\")\n",
    "                        tid = extract_tender_id_from_text(a_with_title.get_attribute(\"title\") or \"\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                # try idEvl param fallback\n",
    "                if not tid:\n",
    "                    parsed = urlparse(href)\n",
    "                    m = re.search(r\"idEvl=([^&]+)\", parsed.query)\n",
    "                    if m:\n",
    "                        tid = re.sub(r\"[^\\w\\-_\\.]\", \"_\", m.group(1))\n",
    "\n",
    "                tid = tid or \"\"\n",
    "\n",
    "                # probe detail page to extract file_name (safe: opens new tab then closes it)\n",
    "                file_name = None\n",
    "                try:\n",
    "                    file_name = extract_file_name_from_detail(driver, href, wait_seconds=1.2)\n",
    "                except Exception as e:\n",
    "                    logger.debug(\"extract_file_name_from_detail failed: %s\", e)\n",
    "\n",
    "                file_name = file_name or \"\"\n",
    "\n",
    "                record = (file_name, tid, href)\n",
    "                if record not in collected:\n",
    "                    collected.append(record)\n",
    "                    logger.info(\"Collected: %s  (file_name=%s tender_id=%s)\", href, file_name, tid)\n",
    "            except Exception as e:\n",
    "                logger.debug(\"Row parse error: %s\", e)\n",
    "\n",
    "        # capture first-row text for change detection\n",
    "        try:\n",
    "            rs2 = rows_selector_try(driver)\n",
    "            last_first_row_text = rs2[0].text.strip() if rs2 and rs2[0].text else None\n",
    "        except Exception:\n",
    "            last_first_row_text = None\n",
    "\n",
    "        if page >= max_pages:\n",
    "            logger.info(\"Reached max pages (%d). Stopping.\", max_pages)\n",
    "            break\n",
    "\n",
    "        clicked = click_next_exact(driver)\n",
    "        if not clicked:\n",
    "            logger.info(\"Next button not found or not clickable. Stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        # wait until first row changes (indicating new page)\n",
    "        try:\n",
    "            def first_row_changed(drv):\n",
    "                try:\n",
    "                    rs = rows_selector_try(drv)\n",
    "                    if not rs: return False\n",
    "                    cur = rs[0].text.strip() if rs[0].text else \"\"\n",
    "                    return cur != (last_first_row_text or \"\")\n",
    "                except Exception:\n",
    "                    return False\n",
    "            WebDriverWait(driver, 10).until(first_row_changed)\n",
    "            time.sleep(0.6)\n",
    "        except TimeoutException:\n",
    "            logger.warning(\"Page didn't change after next click; may be end. Continuing/Breaking.\")\n",
    "            try:\n",
    "                rsnow = rows_selector_try(driver)\n",
    "                cur2 = rsnow[0].text.strip() if rsnow and rsnow[0].text else \"\"\n",
    "                if cur2 == (last_first_row_text or \"\"):\n",
    "                    logger.info(\"No change detected after clicking next; stopping.\")\n",
    "                    break\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "        time.sleep(WAIT)\n",
    "\n",
    "    driver.quit()\n",
    "    logger.info(\"Collected %d URL rows matching '%s'\", len(collected), query)\n",
    "    return collected\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rows = collect_detail_urls(SEARCH_URL, SEARCH_QUERY, max_pages=MAX_PAGES, headless=HEADLESS)\n",
    "    print(\"---- Collected detail URLs ----\")\n",
    "    for fname, tid, u in rows:\n",
    "        print(f\"{fname}\\t{tid}\\t{u}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    try:\n",
    "        with open(OUTPUT_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as fh:\n",
    "            writer = csv.writer(fh)\n",
    "            writer.writerow([\"file_name\", \"tender_id\", \"detail_url\"])\n",
    "            for fname, tid, u in rows:\n",
    "                writer.writerow([fname, tid, u])\n",
    "        logger.info(\"Saved collected URLs to %s (%d rows)\", OUTPUT_CSV, len(rows))\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Failed to write output CSV %s : %s\", OUTPUT_CSV, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f90ca4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Download Adjudicación HTML documents for rows in an input CSV.\n",
    "Input CSV columns: file_name, tender_id, detail_url\n",
    "\n",
    "Outputs:\n",
    " - downloaded_adjudicacion/   (downloaded html files)\n",
    " - download_log.csv           (status log)\n",
    "\n",
    "This variant SKIPS PDFs (and other non-HTML binary content).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, unquote, urlparse\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# -----------------------\n",
    "# Configuration\n",
    "# -----------------------\n",
    "CSV_INPUT = Path(\"collected_adalimumab_urls.csv\")    # change if needed\n",
    "HTML_FOLDER = Path(\"html_pages\")      # optional local saved detail pages, filenames like 2025_010027.html\n",
    "OUTPUT_FOLDER = Path(\"downloaded_adjudicacion\")\n",
    "DOWNLOAD_LOG = Path(\"download_log.csv\")\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\"User-Agent\": \"Mozilla/5.0 (compatible; adjudication-downloader/1.1)\"})\n",
    "\n",
    "# If True, re-download and overwrite files even if they exist; otherwise skip existing files\n",
    "OVERWRITE = False\n",
    "\n",
    "# polite pause between requests\n",
    "REQUEST_DELAY = 0.5\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def load_local_or_fetch_html(local_path: Path, url: str) -> str:\n",
    "    \"\"\"Return HTML content from local file if exists, otherwise fetch from url.\"\"\"\n",
    "    if local_path and local_path.exists():\n",
    "        logging.info(\"Loading local file: %s\", local_path)\n",
    "        return local_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    # fetch\n",
    "    logging.info(\"Fetching detail page: %s\", url)\n",
    "    r = SESSION.get(url, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def find_adjudicacion_link(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Heuristic search for the Adjudicación link:\n",
    "    - first look for anchors containing GetDocumentByIdServlet / FileSystem\n",
    "    - fallback: find text nodes with 'Adjudicación' and find next <a>\n",
    "    - final fallback: first <a> with an <img alt=\"Documento html\"> or similar\n",
    "    \"\"\"\n",
    "    # 1) direct anchors (most reliable)\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if (\"GetDocumentByIdServlet\" in href) or (\"GetDocumentById\" in href) or (\"FileSystem\" in href):\n",
    "            context = (a.get_text(\" \", strip=True) or \"\") + \" \" + (a.find_parent().get_text(\" \", strip=True) if a.find_parent() else \"\")\n",
    "            if re.search(r\"adjudic\", context, re.I) or \"documento\" in context.lower():\n",
    "                return href\n",
    "\n",
    "    # 2) find text nodes containing 'Adjudicación' and look for next anchor\n",
    "    candidates = soup.find_all(text=re.compile(r\"Adjudicaci[oó]n|Adjudicacion\", re.I))\n",
    "    for t in candidates:\n",
    "        el = t.parent\n",
    "        a = el.find_next(\"a\", href=True)\n",
    "        if a:\n",
    "            return a[\"href\"]\n",
    "        a2 = el.find(\"a\", href=True)\n",
    "        if a2:\n",
    "            return a2[\"href\"]\n",
    "\n",
    "    # 3) image-based anchor (html-icon)\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        img = a.find(\"img\")\n",
    "        if img:\n",
    "            alt = (img.get(\"alt\") or img.get(\"title\") or \"\").lower()\n",
    "            if \"documento html\" in alt or \"html\" in alt:\n",
    "                return a[\"href\"]\n",
    "\n",
    "    return None\n",
    "\n",
    "def safe_filename(tender_id: str, original_name: str, ext: str) -> str:\n",
    "    \"\"\"Create a safe filename using tender_id + a short original name fragment + extension.\"\"\"\n",
    "    orig = re.sub(r'[^A-Za-z0-9_\\-\\.]', '_', original_name)[:60]\n",
    "    tid = re.sub(r'[^A-Za-z0-9_\\-\\.]', '_', tender_id)\n",
    "    return f\"{tid}__{orig}{ext}\"\n",
    "\n",
    "# -----------------------\n",
    "# Core download routine\n",
    "# -----------------------\n",
    "def download_adjudicacion_for_row(file_name: str, tender_id: str, detail_url: str) -> dict:\n",
    "    result = {\n",
    "        \"file_name\": file_name,\n",
    "        \"tender_id\": tender_id,\n",
    "        \"detail_url\": detail_url,\n",
    "        \"target_url\": None,\n",
    "        \"saved_path\": None,\n",
    "        \"content_type\": None,\n",
    "        \"status\": None,\n",
    "        \"error\": None\n",
    "    }\n",
    "    try:\n",
    "        # compute local path for detail page if present\n",
    "        safe_local_name = file_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\") + \".html\"\n",
    "        local_path = HTML_FOLDER / safe_local_name\n",
    "\n",
    "        html = load_local_or_fetch_html(local_path, detail_url)\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        href = find_adjudicacion_link(soup)\n",
    "        if not href:\n",
    "            result[\"status\"] = \"no_adjudicacion_link\"\n",
    "            logging.warning(\"No Adjudicación link found for %s\", tender_id)\n",
    "            return result\n",
    "\n",
    "        # resolve absolute url\n",
    "        target_url = href if href.startswith(\"http\") else urljoin(detail_url, href)\n",
    "        result[\"target_url\"] = target_url\n",
    "        logging.info(\"Found adjudicacion target: %s\", target_url)\n",
    "\n",
    "        # request target\n",
    "        r = SESSION.get(target_url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        ct = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "        result[\"content_type\"] = ct\n",
    "\n",
    "        # Explicitly skip PDFs\n",
    "        if \"pdf\" in ct or target_url.lower().endswith(\".pdf\"):\n",
    "            logging.info(\"Skipping PDF target for %s (Content-Type=%s)\", tender_id, ct)\n",
    "            result[\"status\"] = \"skipped_pdf\"\n",
    "            return result\n",
    "\n",
    "        # Ensure it's HTML (Content-Type has html or body starts with '<')\n",
    "        body_start = (r.text or \"\").lstrip()[:10].lower()\n",
    "        is_html = (\"html\" in ct) or (body_start.startswith(\"<\"))\n",
    "        if not is_html:\n",
    "            logging.info(\"Skipping non-HTML target for %s (Content-Type=%s)\", tender_id, ct)\n",
    "            result[\"status\"] = \"skipped_non_html\"\n",
    "            return result\n",
    "\n",
    "        # Save as .html\n",
    "        parsed = urlparse(target_url)\n",
    "        orig_fragment = parsed.query or Path(parsed.path).name or \"adjudicacion\"\n",
    "        orig_fragment = unquote(orig_fragment)\n",
    "        ext = \".html\"\n",
    "        fname = safe_filename(tender_id or file_name, orig_fragment, ext)\n",
    "        OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "        outpath = OUTPUT_FOLDER / fname\n",
    "\n",
    "        if outpath.exists() and not OVERWRITE:\n",
    "            logging.info(\"File already exists (skip): %s\", outpath)\n",
    "            result[\"saved_path\"] = str(outpath)\n",
    "            result[\"status\"] = \"skipped_exists\"\n",
    "            return result\n",
    "\n",
    "        # write text (utf-8)\n",
    "        with open(outpath, \"w\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "            fh.write(r.text)\n",
    "\n",
    "        result[\"saved_path\"] = str(outpath)\n",
    "        result[\"status\"] = \"downloaded_html\"\n",
    "        logging.info(\"Saved adjudicacion HTML to %s\", outpath)\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Error downloading adjudicacion for %s\", tender_id)\n",
    "        result[\"error\"] = str(e)\n",
    "        result[\"status\"] = \"error\"\n",
    "        return result\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "def main():\n",
    "    if not CSV_INPUT.exists():\n",
    "        logging.error(\"Input CSV not found: %s\", CSV_INPUT)\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(CSV_INPUT, dtype=str).fillna(\"\")\n",
    "    logs = []\n",
    "    for idx, row in df.iterrows():\n",
    "        file_name = row.get(\"file_name\") or row.get(\"FileName\") or row.get(\"filename\") or \"\"\n",
    "        tender_id = row.get(\"tender_id\") or row.get(\"tenderId\") or row.get(\"tender\") or file_name\n",
    "        detail_url = row.get(\"detail_url\") or row.get(\"detailUrl\") or row.get(\"url\") or \"\"\n",
    "        if not detail_url:\n",
    "            logging.warning(\"No detail_url for row idx=%s; skipping\", idx)\n",
    "            logs.append({\n",
    "                \"file_name\": file_name,\n",
    "                \"tender_id\": tender_id,\n",
    "                \"detail_url\": detail_url,\n",
    "                \"status\": \"no_detail_url\",\n",
    "                \"error\": \"no detail_url provided\"\n",
    "            })\n",
    "            continue\n",
    "        res = download_adjudicacion_for_row(file_name.strip(), tender_id.strip(), detail_url.strip())\n",
    "        logs.append(res)\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "    pd.DataFrame(logs).to_csv(DOWNLOAD_LOG, index=False, encoding=\"utf-8-sig\")\n",
    "    logging.info(\"All done. Log written to %s\", DOWNLOAD_LOG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1d73e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag,Doctype\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_tender_and_pubdate_from_soup(soup):\n",
    "    \"\"\"\n",
    "    Accepts a BeautifulSoup object or Tag and returns:\n",
    "      { \"tender_id\": str or None, \"publication_date\": \"YYYY-MM-DD\" or None }\n",
    "\n",
    "    If a string is passed accidentally, it will be parsed into a BeautifulSoup.\n",
    "    \"\"\"\n",
    "    # allow passing either a BS object/Tag or raw HTML string\n",
    "    if isinstance(soup, str):\n",
    "        soup = BeautifulSoup(soup, \"html.parser\")\n",
    "    if not isinstance(soup, (BeautifulSoup, Tag)):\n",
    "        raise TypeError(\"soup must be a BeautifulSoup or Tag or HTML string\")\n",
    "\n",
    "    h5 = soup.find(\"h5\")\n",
    "    if not h5:\n",
    "        return {\"tender_id\": None, \"publication_date\": None}\n",
    "\n",
    "    # --- tender id: prefer <strong> inside <h5>, fallback to text between labels ---\n",
    "    tender_id = None\n",
    "    strong = h5.find(\"strong\")\n",
    "    if strong and strong.get_text(strip=True):\n",
    "        tender_id = strong.get_text(strip=True)\n",
    "    else:\n",
    "        # fallback: text between \"Número de Expediente\" and \"Publicado\"\n",
    "        full = h5.get_text(\" \", strip=True)\n",
    "        if \"Número de Expediente\" in full and \"Publicado\" in full:\n",
    "            between = full.split(\"Número de Expediente\", 1)[1].split(\"Publicado\", 1)[0]\n",
    "            tender_id = between.strip() or None\n",
    "\n",
    "    # --- publication date: scan tokens for dd-mm-yyyy or dd/mm/yyyy without using regex ---\n",
    "    pub_date_iso = None\n",
    "    full_text = h5.get_text(\" \", strip=True)\n",
    "\n",
    "    for tok in full_text.split():\n",
    "        t = tok.strip(\" ,.;:()[]\")\n",
    "        t_norm = t.replace(\"/\", \"-\")\n",
    "        # look for tokens with exactly two separators (e.g. 04-05-2023)\n",
    "        if t_norm.count(\"-\") == 2:\n",
    "            parts = t_norm.split(\"-\")\n",
    "            if all(part.isdigit() for part in parts) and 1 <= len(parts[0]) <= 2 and 1 <= len(parts[1]) <= 2 and len(parts[2]) in (2,4):\n",
    "                day, month, year = parts\n",
    "                # try parsing; prefer 4-digit year format\n",
    "                fmt_candidates = [\"%d-%m-%Y\", \"%d-%m-%y\"] if len(year) == 4 else [\"%d-%m-%y\", \"%d-%m-%Y\"]\n",
    "                parsed = None\n",
    "                for fmt in fmt_candidates:\n",
    "                    try:\n",
    "                        parsed = datetime.strptime(f\"{day}-{month}-{year}\", fmt)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                if parsed:\n",
    "                    pub_date_iso = parsed.strftime(\"%Y-%m-%d\")\n",
    "                    break\n",
    "\n",
    "    return {\"tender_id\": tender_id, \"publication_date\": pub_date_iso}\n",
    "\n",
    "from typing import Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "def extract_issuing_authority(soup_input: Union[BeautifulSoup, Tag, str]) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract issuing authority info from the \"Entidad Adjudicadora\" block.\n",
    "\n",
    "    This version ensures that the \"Contacto\" block used for phone/fax/email is\n",
    "    taken from the content *under* the \"Entidad Adjudicadora\" section (so if there\n",
    "    are multiple \"Contacto\" blocks elsewhere, those are ignored).\n",
    "\n",
    "    Args:\n",
    "      soup_input: BeautifulSoup / Tag or raw HTML string containing the fragment.\n",
    "\n",
    "    Returns:\n",
    "      Dict with keys:\n",
    "        - issuing_authority_name\n",
    "        - issuing_authority_type_label\n",
    "        - issuing_authority_type\n",
    "        - issuing_authority_website_label\n",
    "        - issuing_authority_website\n",
    "        - issuing_authority_phone\n",
    "        - issuing_authority_fax\n",
    "        - issuing_authority_email\n",
    "    \"\"\"\n",
    "    # normalize input\n",
    "    if isinstance(soup_input, str):\n",
    "        soup = BeautifulSoup(soup_input, \"html.parser\")\n",
    "    elif isinstance(soup_input, (BeautifulSoup, Tag)):\n",
    "        soup = soup_input\n",
    "    else:\n",
    "        raise TypeError(\"soup_input must be BeautifulSoup, Tag or HTML string\")\n",
    "\n",
    "    def text_or_none(el):\n",
    "        return el.get_text(\" \", strip=True) if el else None\n",
    "\n",
    "    result: Dict[str, Optional[str]] = {\n",
    "        \"issuing_authority_name\": None,\n",
    "        \"issuing_authority_type_label\": None,\n",
    "        \"issuing_authority_type\": None,\n",
    "        \"issuing_authority_website_label\": None,\n",
    "        \"issuing_authority_website\": None,\n",
    "        \"issuing_authority_phone\": None,\n",
    "        \"issuing_authority_fax\": None,\n",
    "        \"issuing_authority_email\": None,\n",
    "    }\n",
    "\n",
    "    # Find the \"Entidad Adjudicadora\" header and the following <ul>\n",
    "    ent_h3 = soup.find(lambda t: t.name == \"h3\" and \"Entidad Adjudicadora\" in t.get_text())\n",
    "    ent_ul = ent_h3.find_next_sibling(\"ul\") if ent_h3 else None\n",
    "\n",
    "    if ent_ul:\n",
    "        for li in ent_ul.find_all(\"li\", recursive=False):\n",
    "            # Name: first li contains <strong> inside a div.noremarca\n",
    "            strong = li.find(\"strong\")\n",
    "            if strong:\n",
    "                result[\"issuing_authority_name\"] = text_or_none(strong)\n",
    "                continue\n",
    "\n",
    "            # Label / value rows: label in <span>, value either in div.noremarca or after span\n",
    "            span = li.find(\"span\")\n",
    "            label = text_or_none(span) if span else None\n",
    "\n",
    "            # Try to get a href if present (for website)\n",
    "            a = li.find(\"a\")\n",
    "            if a and a.has_attr(\"href\"):\n",
    "                value = a[\"href\"].strip()\n",
    "            else:\n",
    "                # prefer div.noremarca\n",
    "                value_div = li.find(\"div\", class_=\"noremarca\")\n",
    "                if value_div:\n",
    "                    value = text_or_none(value_div)\n",
    "                else:\n",
    "                    # fallback: li text minus the label\n",
    "                    full = li.get_text(\" \", strip=True)\n",
    "                    value = full.replace(label, \"\", 1).strip() if label else full\n",
    "\n",
    "            if label and \"Tipo de Administración\" in label:\n",
    "                result[\"issuing_authority_type_label\"] = label\n",
    "                result[\"issuing_authority_type\"] = value or None\n",
    "            elif label and \"Sitio Web\" in label:\n",
    "                result[\"issuing_authority_website_label\"] = label\n",
    "                result[\"issuing_authority_website\"] = value or None\n",
    "\n",
    "    # ---- Find the Contacto block that is under the Entidad Adjudicadora section ----\n",
    "    contact_ul = None\n",
    "    if ent_h3:\n",
    "        # gather siblings after ent_h3 until the next h3 (boundary)\n",
    "        nodes = []\n",
    "        for sib in ent_h3.next_siblings:\n",
    "            # stop if we hit another h3 (new section)\n",
    "            if isinstance(sib, Tag) and sib.name == \"h3\":\n",
    "                break\n",
    "            nodes.append(sib)\n",
    "\n",
    "        # build a small fragment to search inside the Entidad Adjudicadora content\n",
    "        fragment_html = \"\".join(str(n) for n in nodes)\n",
    "        fragment = BeautifulSoup(fragment_html, \"html.parser\")\n",
    "\n",
    "        # Prefer an explicit Contacto <h5> within this fragment\n",
    "        contact_h5 = fragment.find(lambda t: t.name in (\"h5\", \"h3\") and \"Contacto\" in t.get_text())\n",
    "        contact_ul = contact_h5.find_next_sibling(\"ul\") if contact_h5 else None\n",
    "\n",
    "        # If there was no explicit h5 Contacto but there's a .rigCol / .leftCol with Contacto inside, try finding ul directly\n",
    "        if not contact_ul:\n",
    "            # common pattern: a \"rigCol\" or \"leftCol\" section contains contact info\n",
    "            possible = fragment.find(lambda t: isinstance(t, Tag) and (t.get(\"class\") and (\"rigCol\" in t.get(\"class\") or \"leftCol\" in t.get(\"class\"))))\n",
    "            if possible:\n",
    "                contact_ul = possible.find(\"ul\")\n",
    "\n",
    "    # If ent_h3 not found, fall back to global Contacto search (maintain backward compatibility)\n",
    "    if contact_ul is None:\n",
    "        contact_h5_global = soup.find(lambda t: t.name in (\"h5\", \"h3\") and \"Contacto\" in t.get_text())\n",
    "        contact_ul = contact_h5_global.find_next_sibling(\"ul\") if contact_h5_global else None\n",
    "\n",
    "    # Extract phone/fax/email from the located contact_ul\n",
    "    if contact_ul:\n",
    "        for li in contact_ul.find_all(\"li\", recursive=False):\n",
    "            span = li.find(\"span\")\n",
    "            label = text_or_none(span) if span else None\n",
    "\n",
    "            value_div = li.find(\"div\", class_=\"noremarca\")\n",
    "            if value_div:\n",
    "                value = text_or_none(value_div)\n",
    "            else:\n",
    "                full = li.get_text(\" \", strip=True)\n",
    "                value = full.replace(label, \"\", 1).strip() if label else full\n",
    "\n",
    "            if label and \"Teléfono\" in label:\n",
    "                result[\"issuing_authority_phone\"] = value or None\n",
    "            elif label and \"Fax\" in label:\n",
    "                result[\"issuing_authority_fax\"] = value or None\n",
    "            elif label and \"Correo Electrónico\" in label:\n",
    "                result[\"issuing_authority_email\"] = value or None\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage (comment out when embedding):\n",
    "# soup = BeautifulSoup(html_fragment, \"html.parser\")\n",
    "# print(extract_issuing_authority(soup))\n",
    "\n",
    "from typing import Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from datetime import datetime, timedelta, date\n",
    "import calendar\n",
    "import re\n",
    "\n",
    "def extract_contract_details(soup_input: Union[BeautifulSoup, Tag, str]) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract contract details from the 'Objeto del Contrato' block.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "      - tender_title\n",
    "      - estimated_contract_value\n",
    "      - cpv_classification    (list of CPV code strings, or None)\n",
    "      - contract_duration     (exact string as found under \"Plazo de Ejecución\", or None)\n",
    "\n",
    "    NOTE: contract_from and contract_till removed as requested.\n",
    "    \"\"\"\n",
    "    # --- normalize input ---\n",
    "    if isinstance(soup_input, str):\n",
    "        soup = BeautifulSoup(soup_input, \"html.parser\")\n",
    "    elif isinstance(soup_input, (BeautifulSoup, Tag)):\n",
    "        soup = soup_input\n",
    "    else:\n",
    "        raise TypeError(\"soup_input must be BeautifulSoup, Tag or an HTML string\")\n",
    "\n",
    "    def text_or_none(el):\n",
    "        return el.get_text(\" \", strip=True) if el else None\n",
    "\n",
    "    out = {\n",
    "        \"tender_title\": None,\n",
    "        \"estimated_contract_value\": None,\n",
    "        \"cpv_classification\": None,   # will be a list of codes when present\n",
    "        \"contract_duration\": None,\n",
    "    }\n",
    "\n",
    "    # --- 1) Tender title ---\n",
    "    h2 = soup.find(lambda t: t.name == \"h2\" and \"Objeto del Contrato\" in t.get_text())\n",
    "    if h2:\n",
    "        dd = h2.find(\"div\", class_=\"noremarca\")\n",
    "        if dd and dd.get_text(strip=True):\n",
    "            out[\"tender_title\"] = dd.get_text(\" \", strip=True)\n",
    "\n",
    "    # --- 2) Estimated contract value ---\n",
    "    val_li = soup.find(lambda t: t.name == \"li\" and t.find(\"span\") and \"Valor estimado del contrato\" in t.find(\"span\").get_text())\n",
    "    if val_li:\n",
    "        val_div = val_li.find(\"div\", class_=\"noremarca\")\n",
    "        if val_div and val_div.get_text(strip=True):\n",
    "            out[\"estimated_contract_value\"] = val_div.get_text(\" \", strip=True)\n",
    "        else:\n",
    "            span = val_li.find(\"span\")\n",
    "            full = val_li.get_text(\" \", strip=True)\n",
    "            label = span.get_text(\" \", strip=True) if span else \"\"\n",
    "            val = full.replace(label, \"\", 1).strip()\n",
    "            out[\"estimated_contract_value\"] = val if val else None\n",
    "\n",
    "    # --- 3) CPV classification: extract only codes from the Objeto del Contrato container ---\n",
    "    cpv_codes = []\n",
    "    if h2:\n",
    "        container = h2.find_parent(\"div\", class_=\"boxWithBackground\") or h2.find_parent(\"div\", class_=\"box01\") or h2.parent\n",
    "        if container:\n",
    "            candidates = []\n",
    "            cpv_span = container.find(lambda t: t.name == \"span\" and \"Clasificación CPV\" in t.get_text())\n",
    "            if cpv_span:\n",
    "                node = cpv_span.parent\n",
    "                ul = node.find_next_sibling(\"ul\") or node.find_next(\"ul\")\n",
    "                if ul:\n",
    "                    for div in ul.find_all(\"div\", class_=\"noremarca\"):\n",
    "                        txt = div.get_text(\" \", strip=True)\n",
    "                        if txt:\n",
    "                            candidates.append(txt)\n",
    "                else:\n",
    "                    for div in container.find_all(\"div\", class_=\"noremarca\"):\n",
    "                        txt = div.get_text(\" \", strip=True)\n",
    "                        if txt:\n",
    "                            candidates.append(txt)\n",
    "            else:\n",
    "                for div in container.find_all(\"div\", class_=\"noremarca\"):\n",
    "                    txt = div.get_text(\" \", strip=True)\n",
    "                    if txt:\n",
    "                        candidates.append(txt)\n",
    "\n",
    "            seen = set()\n",
    "            for c in candidates:\n",
    "                found = re.findall(r'\\b\\d{8}\\b', c)\n",
    "                if not found:\n",
    "                    found = re.findall(r'\\b\\d{6,8}\\b', c)\n",
    "                for code in found:\n",
    "                    if code not in seen:\n",
    "                        cpv_codes.append(code)\n",
    "                        seen.add(code)\n",
    "\n",
    "    out[\"cpv_classification\"] = cpv_codes if cpv_codes else None\n",
    "\n",
    "    # --- 4) Contract duration: copy the exact string under \"Plazo de Ejecución\" ---\n",
    "    contract_duration_raw = None\n",
    "    # operate within the same container used above (nearest Objeto del Contrato)\n",
    "    if h2:\n",
    "        container = h2.find_parent(\"div\", class_=\"boxWithBackground\") or h2.find_parent(\"div\", class_=\"box01\") or h2.parent\n",
    "    else:\n",
    "        container = soup\n",
    "\n",
    "    # look for a span containing \"Plazo de Ejecución\" (allow accent/no-accent)\n",
    "    plazo_span = None\n",
    "    if container:\n",
    "        # check common exact text\n",
    "        plazo_span = container.find(lambda t: t.name == \"span\" and \"Plazo de Ejecución\" in t.get_text())\n",
    "        if not plazo_span:\n",
    "            # try without accent (some variants)\n",
    "            plazo_span = container.find(lambda t: t.name == \"span\" and \"Plazo de Ejecucion\" in t.get_text())\n",
    "        if not plazo_span:\n",
    "            # try a shorter fingerprint\n",
    "            plazo_span = container.find(lambda t: t.name == \"span\" and \"Plazo\" in t.get_text() and \"Ejec\" in t.get_text())\n",
    "\n",
    "    if plazo_span:\n",
    "        # usual structure: span -> parent li -> following ul with li/div.noremarca holding the duration\n",
    "        parent_li = plazo_span.find_parent(\"li\")\n",
    "        candidates = []\n",
    "        if parent_li:\n",
    "            # direct divs inside the li\n",
    "            for div in parent_li.find_all(\"div\", class_=\"noremarca\", recursive=True):\n",
    "                txt = div.get_text(\" \", strip=True)\n",
    "                if txt:\n",
    "                    candidates.append(txt)\n",
    "            # also check the next sibling UL if present\n",
    "            nxt_ul = parent_li.find_next_sibling(\"ul\")\n",
    "            if nxt_ul:\n",
    "                for div in nxt_ul.find_all(\"div\", class_=\"noremarca\"):\n",
    "                    txt = div.get_text(\" \", strip=True)\n",
    "                    if txt:\n",
    "                        candidates.append(txt)\n",
    "            # sometimes divs are after the li as siblings (example HTML)\n",
    "            for sib in parent_li.find_next_siblings():\n",
    "                if isinstance(sib, Tag) and sib.name in (\"li\", \"h4\", \"h5\", \"h3\"):\n",
    "                    break\n",
    "                if isinstance(sib, Tag):\n",
    "                    for div in sib.find_all(\"div\", class_=\"noremarca\"):\n",
    "                        txt = div.get_text(\" \", strip=True)\n",
    "                        if txt:\n",
    "                            candidates.append(txt)\n",
    "        else:\n",
    "            # fallback: look in next siblings of the span\n",
    "            for node in plazo_span.next_siblings:\n",
    "                if isinstance(node, Tag):\n",
    "                    for div in node.find_all(\"div\", class_=\"noremarca\"):\n",
    "                        txt = div.get_text(\" \", strip=True)\n",
    "                        if txt:\n",
    "                            candidates.append(txt)\n",
    "\n",
    "        # pick the first candidate that looks like a duration (contains digits and a letter)\n",
    "        for c in candidates:\n",
    "            if c and re.search(r'\\d', c):\n",
    "                # strip punctuation/newlines\n",
    "                cleaned = c.strip()\n",
    "                # if the string contains a leading \":\" or similar, remove it\n",
    "                cleaned = re.sub(r'^[\\:\\-\\s]+', '', cleaned)\n",
    "                contract_duration_raw = cleaned\n",
    "                break\n",
    "\n",
    "    # If still not found, fallback: search container for div.noremarca matching patterns like 'NNN Día' etc.\n",
    "    if not contract_duration_raw and container:\n",
    "        for div in container.find_all(\"div\", class_=\"noremarca\"):\n",
    "            txt = div.get_text(\" \", strip=True)\n",
    "            if txt and re.search(r'\\b\\d+\\b', txt) and re.search(r'(día|día\\(s\\)|día\\(s\\)|Día|Día\\(s\\)|día|mes|año|día\\(s\\))', txt, flags=re.IGNORECASE):\n",
    "                contract_duration_raw = txt.strip()\n",
    "                break\n",
    "\n",
    "    # final assign (exact string found or None)\n",
    "    out[\"contract_duration\"] = contract_duration_raw or None\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "from typing import Optional, Dict, Union\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "def extract_award_details(soup_input: Union[BeautifulSoup, Tag, str]) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract award/winner details from the 'Adjudicatario' / award block.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"winning_company\": str or None,\n",
    "        \"winning_company_nif\": str or None,\n",
    "        \"final_contract_value\": str or None,  # value text from \"Importe total ofertado (sin impuestos)\"\n",
    "        \"motivation\": str or None,\n",
    "        \"award_date\": \"YYYY-MM-DD\" or None,\n",
    "        \"award_date_raw\": str or None\n",
    "      }\n",
    "    \"\"\"\n",
    "    # --- normalize input ---\n",
    "    if isinstance(soup_input, str):\n",
    "        soup = BeautifulSoup(soup_input, \"html.parser\")\n",
    "    elif isinstance(soup_input, (BeautifulSoup, Tag)):\n",
    "        soup = soup_input\n",
    "    else:\n",
    "        raise TypeError(\"soup_input must be BeautifulSoup, Tag or HTML string\")\n",
    "\n",
    "    def text_or_none(el):\n",
    "        return el.get_text(\" \", strip=True) if el else None\n",
    "\n",
    "    out: Dict[str, Optional[str]] = {\n",
    "        \"winning_company\": None,\n",
    "        \"winning_company_nif\": None,\n",
    "        \"final_contract_value\": None,\n",
    "        \"motivation\": None,\n",
    "        \"award_date\": None,\n",
    "        \"award_date_raw\": None,\n",
    "    }\n",
    "\n",
    "    # find the Adjudicatario block (h4 \"Adjudicatario\" inside a boxWithBackground)\n",
    "    adjud_h4 = soup.find(lambda t: t.name == \"h4\" and \"Adjudicatario\" in t.get_text())\n",
    "    container = adjud_h4.find_parent(\"div\", class_=\"boxWithBackground\") if adjud_h4 else None\n",
    "    if not container:\n",
    "        # fallback: search any boxWithBackground that contains an h4 with \"Adjudicatario\"\n",
    "        container = soup.find(\"div\", class_=\"boxWithBackground\")\n",
    "\n",
    "    # 1) Winning company: first <strong> inside this container\n",
    "    if container:\n",
    "        strong = container.find(\"strong\")\n",
    "        if strong:\n",
    "            out[\"winning_company\"] = text_or_none(strong)\n",
    "\n",
    "    # 2) NIF: find li where span == \"NIF\"\n",
    "    nif_li = container.find(lambda t: t.name == \"li\" and t.find(\"span\") and \"NIF\" in t.find(\"span\").get_text()) if container else None\n",
    "    if nif_li:\n",
    "        # value usually in div.noremarca\n",
    "        val = nif_li.find(\"div\", class_=\"noremarca\")\n",
    "        out[\"winning_company_nif\"] = text_or_none(val) if val else None\n",
    "\n",
    "    # 3) Final contract value: \"Importe total ofertado (sin impuestos)\"\n",
    "    val_li = container.find(lambda t: t.name == \"li\" and t.find(\"span\") and \"Importe total ofertado (sin impuestos)\" in t.find(\"span\").get_text()) if container else None\n",
    "    if val_li:\n",
    "        vdiv = val_li.find(\"div\", class_=\"noremarca\")\n",
    "        if vdiv:\n",
    "            vtxt = text_or_none(vdiv)\n",
    "            # tidy: remove trailing dots and extra whitespace\n",
    "            if vtxt:\n",
    "                vtxt = vtxt.rstrip(\". \").strip()\n",
    "            out[\"final_contract_value\"] = vtxt or None\n",
    "\n",
    "    # 4) Motivation: find the \"Motivación\" h5 or the li with span \"Motivación\"\n",
    "    motiv_h5 = container.find(lambda t: t.name == \"h5\" and \"Motivación\" in t.get_text()) if container else None\n",
    "    motiv_li = None\n",
    "    if motiv_h5:\n",
    "        motiv_block = motiv_h5.find_next_sibling(\"ul\")\n",
    "        if motiv_block:\n",
    "            motiv_li = motiv_block.find(lambda t: t.name == \"li\" and t.find(\"span\") and \"Motivación\" in t.find(\"span\").get_text())\n",
    "    else:\n",
    "        # fallback: search container for a li span \"Motivación\"\n",
    "        motiv_li = container.find(lambda t: t.name == \"li\" and t.find(\"span\") and \"Motivación\" in t.find(\"span\").get_text()) if container else None\n",
    "\n",
    "    if motiv_li:\n",
    "        v = motiv_li.find(\"div\", class_=\"noremarca\")\n",
    "        out[\"motivation\"] = text_or_none(v) if v else None\n",
    "\n",
    "    # 5) Award date: find the li with span \"Fecha del Acuerdo\"\n",
    "    date_li = container.find(lambda t: t.name == \"li\" and t.find(\"span\") and \"Fecha del Acuerdo\" in t.find(\"span\").get_text()) if container else None\n",
    "    if date_li:\n",
    "        ddiv = date_li.find(\"div\", class_=\"noremarca\")\n",
    "        raw = text_or_none(ddiv) if ddiv else None\n",
    "        if raw:\n",
    "            # try to find a date token like DD/MM/YYYY or DD-MM-YYYY\n",
    "            m = re.search(r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})', raw)\n",
    "            if m:\n",
    "                tok = m.group(1)\n",
    "                parsed = None\n",
    "                for fmt in (\"%d/%m/%Y\", \"%d-%m-%Y\", \"%d/%m/%y\", \"%d-%m-%y\"):\n",
    "                    try:\n",
    "                        parsed_dt = datetime.strptime(tok, fmt)\n",
    "                        parsed = parsed_dt.date()\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                if parsed:\n",
    "                    out[\"award_date\"] = parsed.isoformat()\n",
    "                    out[\"award_date_raw\"] = tok\n",
    "                else:\n",
    "                    out[\"award_date_raw\"] = raw\n",
    "            else:\n",
    "                out[\"award_date_raw\"] = raw\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# Example usage:\n",
    "# html = \"\"\" ... your fragment ... \"\"\"\n",
    "# from bs4 import BeautifulSoup\n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "# print(extract_award_details(soup))\n",
    "\n",
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    import glob\n",
    "    import os\n",
    "    import csv\n",
    "    from bs4 import BeautifulSoup\n",
    "    from deep_translator import GoogleTranslator\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "\n",
    "    INPUT_DIR = \"downloaded_adjudicacion\"\n",
    "    GLOB_PATTERN = os.path.join(INPUT_DIR, \"*.html\")\n",
    "    OUTPUT_CSV = \"tender_data_final.csv\"\n",
    "\n",
    "    # translator instance (re-used)\n",
    "    translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
    "\n",
    "    # helper: decide whether to translate a value\n",
    "    date_re = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b')\n",
    "    currency_re = re.compile(r'^[\\s\\d\\.,]+(?:EUR|€|USD|€\\.)?$', re.IGNORECASE)\n",
    "    url_re = re.compile(r'https?://', re.IGNORECASE)\n",
    "    nif_re = re.compile(r'^[A-Z]\\d{7,}$', re.IGNORECASE)  # simple NIF like A08130502\n",
    "    code_re = re.compile(r'^\\d{4,8}$')  # CPV-like single code\n",
    "\n",
    "    def should_translate(val: str) -> bool:\n",
    "        if not val or not isinstance(val, str):\n",
    "            return False\n",
    "        s = val.strip()\n",
    "        if url_re.search(s):\n",
    "            return False\n",
    "        if date_re.search(s) and len(s) <= 20:\n",
    "            return False\n",
    "        if currency_re.match(s):\n",
    "            return False\n",
    "        if nif_re.match(s):\n",
    "            return False\n",
    "        if re.fullmatch(r'[\\d\\.,\\s\\-\\:\\(\\)]+', s):\n",
    "            return False\n",
    "        if code_re.match(s):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # fields/column order: union of keys from all extractors + file name\n",
    "    columns = [\n",
    "        \"source_file\",\n",
    "        # from tender_and_pubdate\n",
    "        \"tender_id\", \"publication_date\",\n",
    "        # from issuing authority\n",
    "        \"issuing_authority_name\", \"issuing_authority_type_label\", \"issuing_authority_type\",\n",
    "        \"issuing_authority_website_label\", \"issuing_authority_website\",\n",
    "        \"issuing_authority_phone\", \"issuing_authority_fax\", \"issuing_authority_email\",\n",
    "        # from contract_details (per contract/lot)\n",
    "        \"tender_title\", \"estimated_contract_value\", \"cpv_classification\", \"contract_duration\",\n",
    "        # from award_details (per adjudicatario)\n",
    "        \"winning_company\", \"winning_company_nif\", \"final_contract_value\",\n",
    "        \"motivation\", \"award_date\", \"award_date_raw\",\n",
    "    ]\n",
    "\n",
    "    records = []\n",
    "\n",
    "    files = sorted(glob.glob(GLOB_PATTERN))\n",
    "    if not files:\n",
    "        print(\"No HTML files found in\", INPUT_DIR)\n",
    "\n",
    "    # progress-aware processing over files\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        files_iter = tqdm(files, desc=\"Processing HTML files\", unit=\"file\")\n",
    "        use_tqdm = True\n",
    "    except Exception:\n",
    "        files_iter = enumerate(files, 1)\n",
    "        use_tqdm = False\n",
    "        total = len(files)\n",
    "\n",
    "    for it in files_iter:\n",
    "        if use_tqdm:\n",
    "            filepath = it\n",
    "        else:\n",
    "            idx, filepath = it\n",
    "            print(f\"Processing {idx}/{total}: {os.path.basename(filepath)}\")\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to open/parse {filepath}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # 1) Global/exact-once extractors\n",
    "        try:\n",
    "            tender_global = extract_tender_and_pubdate_from_soup(soup) or {}\n",
    "        except Exception as e:\n",
    "            print(f\"extract_tender_and_pubdate_from_soup failed for {filepath}: {e}\")\n",
    "            tender_global = {}\n",
    "\n",
    "        try:\n",
    "            issuing_global = extract_issuing_authority(soup) or {}\n",
    "        except Exception as e:\n",
    "            print(f\"extract_issuing_authority failed for {filepath}: {e}\")\n",
    "            issuing_global = {}\n",
    "\n",
    "        # 2) Find all contract containers (those containing an h2 \"Objeto del Contrato\")\n",
    "        contract_containers = []\n",
    "        for tag in soup.find_all():\n",
    "            if tag.name and tag.find(lambda t: t.name == \"h2\" and \"Objeto del Contrato\" in t.get_text()):\n",
    "                # choose the highest-level container that encloses the h2: prefer div.boxWithBackground or div.box01\n",
    "                # the tag itself may be the h2's parent; promote to nearest ancestor with class boxWithBackground/box01\n",
    "                h2 = tag.find(lambda t: t.name == \"h2\" and \"Objeto del Contrato\" in t.get_text())\n",
    "                container = h2.find_parent(\"div\", class_=\"boxWithBackground\") or h2.find_parent(\"div\", class_=\"box01\") or h2.parent\n",
    "                # store container only once\n",
    "                if container and container not in contract_containers:\n",
    "                    contract_containers.append(container)\n",
    "\n",
    "        # 3) Find all adjudicatario containers (those containing an h4 \"Adjudicatario\")\n",
    "        adjud_containers = []\n",
    "        for tag in soup.find_all():\n",
    "            if tag.name and tag.find(lambda t: t.name == \"h4\" and \"Adjudicatario\" in t.get_text()):\n",
    "                h4 = tag.find(lambda t: t.name == \"h4\" and \"Adjudicatario\" in t.get_text())\n",
    "                container = h4.find_parent(\"div\", class_=\"boxWithBackground\") or h4.parent\n",
    "                if container and container not in adjud_containers:\n",
    "                    adjud_containers.append(container)\n",
    "\n",
    "        # Build document order index mapping: enumerate all tags and map id(tag) -> index\n",
    "        doc_tags = list(soup.find_all())\n",
    "        tag_index = {id(t): i for i, t in enumerate(doc_tags)}\n",
    "\n",
    "        # Helper: get index of a container (use first child tag if container not in doc_tags)\n",
    "        def idx_of(tag):\n",
    "            return tag_index.get(id(tag), None)\n",
    "\n",
    "        # If no contract containers found, try fallback: global contract area (use entire soup as single contract)\n",
    "        if not contract_containers:\n",
    "            contract_containers = [soup]\n",
    "\n",
    "        # For each adjud container, find the nearest preceding contract container by document index\n",
    "        for adjud in adjud_containers:\n",
    "            adjud_idx = idx_of(adjud)\n",
    "            # find candidate contract with index < adjud_idx and the largest such index\n",
    "            chosen_contract = None\n",
    "            best_idx = -1\n",
    "            for c in contract_containers:\n",
    "                c_idx = idx_of(c)\n",
    "                if c_idx is None:\n",
    "                    # try to find index via first child\n",
    "                    children = list(c.find_all())\n",
    "                    if children:\n",
    "                        c_idx = idx_of(children[0])\n",
    "                if c_idx is None:\n",
    "                    continue\n",
    "                if adjud_idx is not None and c_idx <= adjud_idx and c_idx > best_idx:\n",
    "                    best_idx = c_idx\n",
    "                    chosen_contract = c\n",
    "            # If not found, fallback to the first contract container\n",
    "            if chosen_contract is None:\n",
    "                chosen_contract = contract_containers[0] if contract_containers else soup\n",
    "\n",
    "            # Extract per-contract details from chosen_contract\n",
    "            try:\n",
    "                contract_details = extract_contract_details(chosen_contract) or {}\n",
    "            except Exception as e:\n",
    "                print(f\"extract_contract_details failed for contract near {filepath}: {e}\")\n",
    "                contract_details = {}\n",
    "\n",
    "            # Extract award details from adjud container (per adjud)\n",
    "            try:\n",
    "                award_details = extract_award_details(adjud) or {}\n",
    "            except Exception as e:\n",
    "                print(f\"extract_award_details failed for adjud near {filepath}: {e}\")\n",
    "                award_details = {}\n",
    "\n",
    "            # Build row merging global + contract + award\n",
    "            row = {k: None for k in columns}\n",
    "            row[\"source_file\"] = os.path.basename(filepath)\n",
    "\n",
    "            # helper that will translate text values when appropriate\n",
    "            def maybe_translate_and_assign(field_name, value):\n",
    "                if value is None:\n",
    "                    row[field_name] = None\n",
    "                    return\n",
    "                # for CPV list, keep as-is (don't translate numeric codes), join into comma string\n",
    "                if field_name == \"cpv_classification\":\n",
    "                    if isinstance(value, list):\n",
    "                        row[field_name] = \", \".join(value) if value else None\n",
    "                    else:\n",
    "                        row[field_name] = value\n",
    "                    return\n",
    "                # do not translate website field\n",
    "                if field_name in (\"issuing_authority_website\",):\n",
    "                    row[field_name] = value\n",
    "                    return\n",
    "                # If value is non-string (e.g., list), coerce to str\n",
    "                if not isinstance(value, str):\n",
    "                    value = str(value)\n",
    "                # decide translation\n",
    "                if should_translate(value):\n",
    "                    try:\n",
    "                        translated = translator.translate(value)\n",
    "                        row[field_name] = translated if translated else value\n",
    "                    except Exception:\n",
    "                        row[field_name] = value\n",
    "                else:\n",
    "                    row[field_name] = value\n",
    "\n",
    "            # tender global\n",
    "            maybe_translate_and_assign(\"tender_id\", tender_global.get(\"tender_id\"))\n",
    "            row[\"publication_date\"] = tender_global.get(\"publication_date\")\n",
    "\n",
    "            # issuing authority (global)\n",
    "            maybe_translate_and_assign(\"issuing_authority_name\", issuing_global.get(\"issuing_authority_name\"))\n",
    "            maybe_translate_and_assign(\"issuing_authority_type_label\", issuing_global.get(\"issuing_authority_type_label\"))\n",
    "            maybe_translate_and_assign(\"issuing_authority_type\", issuing_global.get(\"issuing_authority_type\"))\n",
    "            row[\"issuing_authority_website_label\"] = issuing_global.get(\"issuing_authority_website_label\")\n",
    "            row[\"issuing_authority_website\"] = issuing_global.get(\"issuing_authority_website\")\n",
    "            row[\"issuing_authority_phone\"] = issuing_global.get(\"issuing_authority_phone\")\n",
    "            row[\"issuing_authority_fax\"] = issuing_global.get(\"issuing_authority_fax\")\n",
    "            row[\"issuing_authority_email\"] = issuing_global.get(\"issuing_authority_email\")\n",
    "\n",
    "            # contract details (from chosen_contract)\n",
    "            maybe_translate_and_assign(\"tender_title\", contract_details.get(\"tender_title\"))\n",
    "            row[\"estimated_contract_value\"] = contract_details.get(\"estimated_contract_value\")\n",
    "            maybe_translate_and_assign(\"cpv_classification\", contract_details.get(\"cpv_classification\"))\n",
    "            row[\"contract_duration\"] = contract_details.get(\"contract_duration\")\n",
    "\n",
    "            # award details (from adjud)\n",
    "            maybe_translate_and_assign(\"winning_company\", award_details.get(\"winning_company\"))\n",
    "            row[\"winning_company_nif\"] = award_details.get(\"winning_company_nif\")\n",
    "            row[\"final_contract_value\"] = award_details.get(\"final_contract_value\")\n",
    "            maybe_translate_and_assign(\"motivation\", award_details.get(\"motivation\"))\n",
    "            row[\"award_date\"] = award_details.get(\"award_date\")\n",
    "            row[\"award_date_raw\"] = award_details.get(\"award_date_raw\")\n",
    "\n",
    "            records.append(row)\n",
    "\n",
    "        # If there were contracts but no adjudicatario blocks (rare), create one row per contract using contract details only\n",
    "        if contract_containers and not adjud_containers:\n",
    "            for c in contract_containers:\n",
    "                try:\n",
    "                    contract_details = extract_contract_details(c) or {}\n",
    "                except Exception as e:\n",
    "                    contract_details = {}\n",
    "                row = {k: None for k in columns}\n",
    "                row[\"source_file\"] = os.path.basename(filepath)\n",
    "                maybe_translate_and_assign = lambda fn, val: row.__setitem__(fn, val) if val is not None else None\n",
    "                # global tender/issuing\n",
    "                row[\"tender_id\"] = tender_global.get(\"tender_id\")\n",
    "                row[\"publication_date\"] = tender_global.get(\"publication_date\")\n",
    "                row[\"issuing_authority_name\"] = issuing_global.get(\"issuing_authority_name\")\n",
    "                row[\"issuing_authority_website\"] = issuing_global.get(\"issuing_authority_website\")\n",
    "                # contract fields\n",
    "                row[\"tender_title\"] = contract_details.get(\"tender_title\")\n",
    "                row[\"estimated_contract_value\"] = contract_details.get(\"estimated_contract_value\")\n",
    "                if isinstance(contract_details.get(\"cpv_classification\"), list):\n",
    "                    row[\"cpv_classification\"] = \", \".join(contract_details.get(\"cpv_classification\"))\n",
    "                else:\n",
    "                    row[\"cpv_classification\"] = contract_details.get(\"cpv_classification\")\n",
    "                row[\"contract_duration\"] = contract_details.get(\"contract_duration\")\n",
    "                records.append(row)\n",
    "\n",
    "    # write out to CSV using pandas to preserve headers and quoting\n",
    "    if records:\n",
    "        df = pd.DataFrame(records, columns=columns)\n",
    "        df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Wrote {len(records)} rows to {OUTPUT_CSV}\")\n",
    "    else:\n",
    "        print(\"No records extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bea60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data_cleaning_with_progress.py\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "from typing import Optional\n",
    "\n",
    "INPUT_CSV = \"tender_data_final.csv\"\n",
    "OUTPUT_CSV = \"tender_data_final_cleaned.csv\"\n",
    "\n",
    "# Columns to keep (in this order)\n",
    "KEEP_COLS = [\n",
    "    \"tender_id\",\n",
    "    \"publication_date\",\n",
    "    \"issuing_authority_name\",\n",
    "    \"issuing_authority_type\",\n",
    "    \"issuing_authority_website\",\n",
    "    \"issuing_authority_phone\",\n",
    "    \"issuing_authority_fax\",\n",
    "    \"issuing_authority_email\",\n",
    "    \"tender_title\",\n",
    "    \"estimated_contract_value\",\n",
    "    \"cpv_classification\",\n",
    "    \"contract_duration\",\n",
    "    \"winning_company\",\n",
    "    \"winning_company_nif\",\n",
    "    \"final_contract_value\",\n",
    "    \"motivation\"\n",
    "]\n",
    "\n",
    "# translator (reused)\n",
    "translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
    "\n",
    "# Heuristics to skip translation for fields that are not natural language\n",
    "_url_re = re.compile(r'https?://', re.I)\n",
    "_date_re = re.compile(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b')\n",
    "_currency_like_re = re.compile(r'^[\\s\\d\\.,]+(?:EUR|€|USD|€\\.)?$', re.I)\n",
    "_nif_re = re.compile(r'^[A-Z]\\d{7,}$', re.I)\n",
    "_code_re = re.compile(r'^[\\d,\\s\\.\\-]+$')\n",
    "\n",
    "def should_translate(val: str) -> bool:\n",
    "    if val is None:\n",
    "        return False\n",
    "    if not isinstance(val, str):\n",
    "        return False\n",
    "    s = val.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if _url_re.search(s):\n",
    "        return False\n",
    "    if _date_re.search(s) and len(s) <= 20:\n",
    "        return False\n",
    "    if _currency_like_re.match(s):\n",
    "        return False\n",
    "    if _nif_re.match(s):\n",
    "        return False\n",
    "    if len(s) <= 4 and s.isupper():\n",
    "        return False\n",
    "    if _code_re.fullmatch(s):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# -------------------------\n",
    "# Robust currency parsing utilities (European-aware)\n",
    "def parse_currency_to_float(s: Optional[str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Parse a currency-like string into a float (EUR), with European-format handling.\n",
    "    Examples:\n",
    "      - \"150.457,5 EUR\" -> 150457.5\n",
    "      - \"22.078,8 EUR.\" -> 22078.8\n",
    "      - \"22,078.8 EUR\" -> heuristically handled\n",
    "      - \"22509 EUR\" -> 22509.0\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    # keep only digits, commas, dots and minus sign\n",
    "    s_clean = re.sub(r'[^0-9\\-,\\.]', '', s)\n",
    "    if not s_clean:\n",
    "        return None\n",
    "\n",
    "    # If both '.' and ',' present: treat '.' as thousands, ',' as decimal\n",
    "    if '.' in s_clean and ',' in s_clean:\n",
    "        # remove thousands dots, convert comma decimal to dot\n",
    "        s_num = s_clean.replace('.', '').replace(',', '.')\n",
    "    elif ',' in s_clean and '.' not in s_clean:\n",
    "        # only comma present: treat comma as decimal separator\n",
    "        s_num = s_clean.replace(',', '.')\n",
    "    elif '.' in s_clean and ',' not in s_clean:\n",
    "        # only dot present: ambiguous\n",
    "        # If there's exactly one dot and exactly 3 digits after it, assume thousands (remove the dot)\n",
    "        parts = s_clean.split('.')\n",
    "        if len(parts) == 2 and len(parts[1]) == 3:\n",
    "            s_num = ''.join(parts)\n",
    "        else:\n",
    "            # otherwise treat dot as decimal\n",
    "            s_num = s_clean\n",
    "    else:\n",
    "        s_num = s_clean\n",
    "\n",
    "    # If multiple dots remain, keep last as decimal and remove earlier dots\n",
    "    if s_num.count('.') > 1:\n",
    "        parts = s_num.split('.')\n",
    "        s_num = ''.join(parts[:-1]) + '.' + parts[-1]\n",
    "\n",
    "    # Remove leading/trailing stray characters\n",
    "    s_num = re.sub(r'^[^0-9\\.\\-]+|[^0-9\\.\\-]+$', '', s_num)\n",
    "    if not s_num:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        val = float(s_num)\n",
    "        return val\n",
    "    except Exception:\n",
    "        digits = re.sub(r'[^\\d\\-]', '', s_num)\n",
    "        if digits:\n",
    "            try:\n",
    "                return float(digits)\n",
    "            except Exception:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "def clean_dataframe_with_progress(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # keep only columns that exist and the required ones\n",
    "    present_cols = [c for c in KEEP_COLS if c in df.columns]\n",
    "    df = df[present_cols].copy()\n",
    "\n",
    "    # 1) Normalize tender_title: remove leading colon \":\" and surrounding spaces\n",
    "    if \"tender_title\" in df.columns:\n",
    "        df[\"tender_title\"] = df[\"tender_title\"].astype(str).fillna(\"\")\n",
    "        df[\"tender_title\"] = df[\"tender_title\"].str.replace(r'^\\s*[:：]\\s*', '', regex=True)\n",
    "        df[\"tender_title\"] = df[\"tender_title\"].replace({'': None})\n",
    "\n",
    "    # Prepare output rows\n",
    "    output_rows = []\n",
    "    nrows = len(df)\n",
    "\n",
    "    # Setup progress bar (tqdm if available)\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "        iterator = tqdm(range(nrows), desc=\"Cleaning rows\", unit=\"row\")\n",
    "    except Exception:\n",
    "        iterator = range(nrows)\n",
    "\n",
    "    # Build a translation cache to avoid repeated network calls\n",
    "    translation_cache = {}\n",
    "\n",
    "    text_columns = [\n",
    "        \"issuing_authority_name\", \"issuing_authority_type\",\n",
    "        \"tender_title\", \"cpv_classification\", \"contract_duration\",\n",
    "        \"winning_company\", \"motivation\"\n",
    "    ]\n",
    "\n",
    "    for i in iterator:\n",
    "        row = df.iloc[i]\n",
    "        cleaned_row = {}\n",
    "\n",
    "        # Copy simple fields directly\n",
    "        for col in [\n",
    "            \"tender_id\", \"publication_date\",\n",
    "            \"issuing_authority_website\", \"issuing_authority_phone\",\n",
    "            \"issuing_authority_fax\", \"issuing_authority_email\",\n",
    "            \"winning_company_nif\", \"estimated_contract_value\", \"final_contract_value\",\n",
    "            \"cpv_classification\"\n",
    "        ]:\n",
    "            if col in row.index:\n",
    "                cleaned_row[col] = row[col] if pd.notna(row[col]) and row[col] != \"\" else None\n",
    "            else:\n",
    "                cleaned_row[col] = None\n",
    "\n",
    "        # Textual fields: translate if needed (with cache)\n",
    "        for col in text_columns:\n",
    "            if col in row.index:\n",
    "                val = row[col]\n",
    "                if val is None or (isinstance(val, float) and math.isnan(val)):\n",
    "                    cleaned_row[col] = None\n",
    "                    continue\n",
    "                val = str(val).strip()\n",
    "                if val == \"\":\n",
    "                    cleaned_row[col] = None\n",
    "                    continue\n",
    "                # translation cache\n",
    "                if val in translation_cache:\n",
    "                    cleaned_row[col] = translation_cache[val]\n",
    "                    continue\n",
    "                # decide whether to translate\n",
    "                if should_translate(val):\n",
    "                    try:\n",
    "                        translated = translator.translate(val)\n",
    "                        translated = translated if translated else val\n",
    "                    except Exception:\n",
    "                        translated = val\n",
    "                else:\n",
    "                    translated = val\n",
    "                translation_cache[val] = translated\n",
    "                cleaned_row[col] = translated\n",
    "            else:\n",
    "                cleaned_row[col] = None\n",
    "\n",
    "        # Parse currency numeric values per-row (use European-aware parser)\n",
    "        est_val_raw = row.get(\"estimated_contract_value\") if \"estimated_contract_value\" in row.index else None\n",
    "        fin_val_raw = row.get(\"final_contract_value\") if \"final_contract_value\" in row.index else None\n",
    "        est_num = parse_currency_to_float(est_val_raw) if est_val_raw not in (None, \"\") else None\n",
    "        fin_num = parse_currency_to_float(fin_val_raw) if fin_val_raw not in (None, \"\") else None\n",
    "\n",
    "        # contract_value_raw: numeric max of est and final (float), prefer final if equal\n",
    "        contract_value_raw = None\n",
    "        if est_num is None and fin_num is None:\n",
    "            contract_value_raw = None\n",
    "        elif est_num is None:\n",
    "            contract_value_raw = fin_num\n",
    "        elif fin_num is None:\n",
    "            contract_value_raw = est_num\n",
    "        else:\n",
    "            # choose maximum\n",
    "            contract_value_raw = max(est_num, fin_num)\n",
    "\n",
    "        cleaned_row[\"contract_value_raw\"] = contract_value_raw  # numeric float or None\n",
    "\n",
    "        # Keep contract_duration exactly as the translated/cleaned text above (it was translated earlier)\n",
    "        # contract_duration should be in cleaned_row due to text_columns translation step\n",
    "        # Ensure contract_duration is None when empty\n",
    "        if cleaned_row.get(\"contract_duration\") in (\"\", None):\n",
    "            cleaned_row[\"contract_duration\"] = None\n",
    "\n",
    "        # Final assemble: map to final column names required\n",
    "        out_row = {\n",
    "            \"tender_id\": cleaned_row.get(\"tender_id\"),\n",
    "            \"publication_date\": cleaned_row.get(\"publication_date\"),\n",
    "            \"issuing_authority_name\": cleaned_row.get(\"issuing_authority_name\"),\n",
    "            \"issuing_authority_type\": cleaned_row.get(\"issuing_authority_type\"),\n",
    "            \"issuing_authority_website\": cleaned_row.get(\"issuing_authority_website\"),\n",
    "            \"issuing_authority_phone\": cleaned_row.get(\"issuing_authority_phone\"),\n",
    "            \"issuing_authority_fax\": cleaned_row.get(\"issuing_authority_fax\"),\n",
    "            \"issuing_authority_email\": cleaned_row.get(\"issuing_authority_email\"),\n",
    "            \"tender_title\": cleaned_row.get(\"tender_title\"),\n",
    "            \"estimated_contract_value\": cleaned_row.get(\"estimated_contract_value\"),\n",
    "            \"cpv_classification\": cleaned_row.get(\"cpv_classification\"),\n",
    "            \"contract_duration\": cleaned_row.get(\"contract_duration\"),\n",
    "            \"winning_company\": cleaned_row.get(\"winning_company\"),\n",
    "            \"winning_company_nif\": cleaned_row.get(\"winning_company_nif\"),\n",
    "            \"final_contract_value\": cleaned_row.get(\"final_contract_value\"),\n",
    "            \"motivation\": cleaned_row.get(\"motivation\"),\n",
    "            # numeric float field requested\n",
    "            \"contract_value_raw_EUR\": cleaned_row.get(\"contract_value_raw\"),\n",
    "        }\n",
    "\n",
    "        output_rows.append(out_row)\n",
    "\n",
    "    # Build final DataFrame\n",
    "    df_final = pd.DataFrame(output_rows, columns=[\n",
    "        \"tender_id\",\n",
    "        \"publication_date\",\n",
    "        \"issuing_authority_name\",\n",
    "        \"issuing_authority_type\",\n",
    "        \"issuing_authority_website\",\n",
    "        \"issuing_authority_phone\",\n",
    "        \"issuing_authority_fax\",\n",
    "        \"issuing_authority_email\",\n",
    "        \"tender_title\",\n",
    "        \"estimated_contract_value\",\n",
    "        \"cpv_classification\",\n",
    "        \"contract_duration\",\n",
    "        \"winning_company\",\n",
    "        \"winning_company_nif\",\n",
    "        \"final_contract_value\",\n",
    "        \"motivation\",\n",
    "        \"contract_value_raw_EUR\"\n",
    "    ])\n",
    "\n",
    "    return df_final\n",
    "\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"Loading:\", INPUT_CSV)\n",
    "    df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, na_values=[\"\"])\n",
    "    # convert empty strings to None\n",
    "    df = df.where(df.notnull() & df.astype(bool), None)\n",
    "\n",
    "    cleaned = clean_dataframe_with_progress(df)\n",
    "    cleaned.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(\"Wrote cleaned data to\", OUTPUT_CSV)\n",
    "    print(\"Rows:\", len(cleaned))\n",
    "    print(cleaned.head(5).to_dict(orient=\"records\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
